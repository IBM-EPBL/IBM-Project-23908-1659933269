# -*- coding: utf-8 -*-
"""Parkinson's_Disease_using_Machine_Learning_Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RpEFw_KlGgQb6YDYICYlq1EPtY75tlr6

# Machine Learning Algorithm for Parkinson Disease

#### Importing libaries
"""

import warnings
warnings.filterwarnings("ignore") #Not to display the warnings

import numpy as np 
import pandas as pd
import os, sys
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score #Modelmetrics

pip install lux

import lux

"""## Data preprocessing and Exploratory Data Analysis(EDA)"""

parkinson_data = pd.read_csv('parkinsons.data')
print(parkinson_data)

"""**MDVH** denotes Maximum or Minimum Vocal Fundamental Frequency"""

parkinson_data

parkinson_data.head(n=20)

parkinson_data.tail(50)

parkinson_data.shape
#(rows,columns)

#Capturing for null values if any of it is available
parkinson_data.isnull().sum()

"""No null values are present in the data"""

parkinson_data.describe().round(2).style.background_gradient(cmap='Blues')

parkinson_data.dtypes

for i in parkinson_data:
  print(i)

#Verifying the unique values in the columns
for i in parkinson_data:
  print("##############################",i,"##############################")
  print()
  print(set(parkinson_data[i].tolist()))

parkinson_data['PPE'].tolist()

variable=parkinson_data['status'].value_counts()
variable_data=pd.DataFrame({'status':variable.index,'values':variable.values})
variable_data

"""## Data visualization"""

#Data visualization
import seaborn as sns
import matplotlib.pyplot as plt
variable = parkinson_data["status"].value_counts()
variable_data = pd.DataFrame({'status':variable.index,'values':variable.values})
sns.barplot(x='status',y='values',data=variable_data)

#Analyzing the distribution of the data using distplot
def distplots(col):
  sns.distplot(parkinson_data[col])
  plt.show()

for i in list(parkinson_data.columns)[1:]:
  distplots(i)

sns.distplot(parkinson_data["PPE"])

sns.distplot(parkinson_data['D2'])

#Checking for outliers using boxplot from seabron framework across different quartiles
def boxplots(col):
  sns.boxplot(parkinson_data[col])
  plt.show()

for i in list(parkinson_data.select_dtypes(exclude=["object"]).columns)[1:]:
  boxplots(i)

#Figuring out the correlations using heatmap to visualize between the features and patterns in the data used for this project

plt.figure(figsize=(20,20))
correlation_data=parkinson_data.corr()
sns.heatmap(correlation_data,annot=True)

"""From the above heatmap obtained, we can see the magnitude of a value in a color code ranging from minor to major intensity.As we can analyze from the above result that the color code is denoted from minimum to maximum intensity.

Maximum intensity denotes the decimal value of 1.00 (White) and Minimum intensity starts from -0.75 (Darkblue). The color determines the positive and negative relations using the data.

"""

#We are making the final changes in the data by dividing the data into independent as x and dependent variables as y and removing the ID column
x = parkinson_data.drop(["status","name"],axis=1)
y = parkinson_data["status"]
#It is done to integrate the two x and y variables into the model building steps

#After the changes,let's detect the label balance
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter #For priortizing the importance to store elements as dictionary keys, and their counts as values.
print(Counter(y))

#Now,we are balancing the labels
ROS = RandomOverSampler() #To compensate the imbalance part present in the data
x_ROS,y_ROS = ROS.fit_resample(x, y)
print(Counter(y_ROS))

"""Scaling the data"""

#It is very much important to scale the data for the betterment of the model using such as Support Vector Machine and K Nearest Neighbor Algorithms
Scaler_data = MinMaxScaler((-1,1))
x = Scaler_data.fit_transform(x_ROS)
y = y_ROS

#Now, we are applying feature engineering and Principle Component Analysis using Data Mining for extracting high variance features and transforms
#Mining value from the data
#We are choosing the minimal number of variance as 0.95 as to target that the 95% of the variance is proved or confined from the mining process

from sklearn.decomposition import PCA
Princple_CA = PCA(.95)
X_PCA = Princple_CA.fit_transform(x)
print(x.shape)
print(X_PCA.shape)

"""We have noticed that eight columns are needed to prove the 95 % of the data is retained"""

#Here the Parkinson_data is splitted into training and testing sets by maintaining 20% of the data sample for testing step
x_train,x_test,y_train,y_test = train_test_split(X_PCA,y, test_size=0.2, random_state=7)

"""Since the labels from the data has been balanced so we are to use metrics such as accuracy_score, confusion_matrix, f1_score, precision_score and recall_score

Since we need to get boolean responses after the disease prediction so we are using Logistic Regression by the use of independent variables by assuming that the parkinson_data is linearably separable

## Model Building (Training and Testing)

### Data mining and performance metrics
"""

#We are going to import and use it for assessing the model using performance metrics from Classification process
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
List_metrics = []
List_accuracy = []

#Logistic Regression
from sklearn.linear_model import LogisticRegression
Classification_model = LogisticRegression(C=0.4,max_iter=1000,solver='liblinear')
Log_Regression = Classification_model.fit(x_train, y_train)
y_pred = Classification_model.predict(x_test)  #Prediction
Log_Regression_accuracy = accuracy_score(y_test, y_pred)  #Accuracy
print("The accuracy score with Logistic regression is:",Log_Regression_accuracy)

#Decision Tree Classificaton using supervised machine learning for classifiying the data with confident accuracy
from sklearn.tree import DecisionTreeClassifier
Classification_tree = DecisionTreeClassifier(random_state=14)
Decision_tree = Classification_tree.fit(x_train, y_train)
y_pred2 = Classification_tree.predict(x_test) #Prediction
Dec_tree_accuracy = accuracy_score(y_test, y_pred2) #Accuracy
print("The accuracy score with Decision Tree Classifier is:",Dec_tree_accuracy)

#Random Forest Classifier is used for its high dimensionality and accuracy capabilities, here information gain is priortized
from sklearn.ensemble import RandomForestClassifier
Classification_random = RandomForestClassifier(random_state=14)
RFE = Classification_random.fit(x_train, y_train)
y_pred3 = Classification_random.predict(x_test) #Prediction
Ran_For_accuracy = accuracy_score(y_test, y_pred3) #Accuracy
print("The accuracy score with Random Forest Classifier(Information gain) is:",Ran_For_accuracy)

#Random Forest Classifier with entropy condition
from sklearn.ensemble import RandomForestClassifier
Classification_entropy = RandomForestClassifier(criterion='entropy')
RFE = Classification_entropy.fit(x_train,y_train)
y_pred4 = Classification_entropy.predict(x_test)
Random = accuracy_score(y_test, y_pred4)
print("The accuracy score with Random Forest Classifier(Entropy) is:",Random)

#Using Support Vector Machine (SVM) for to enhance the similarity and to increase the scaling factor of the model
from sklearn.svm import SVC
Parkinson_model = SVC(cache_size=100)
Support_vector_machine = Parkinson_model.fit(x_train, y_train)
y_pred5 = Parkinson_model.predict(x_test)
Support_accuracy = accuracy_score(y_test, y_pred5)
print("The accuracy score with Support Vector Machine is:",Support_accuracy)

#K Nearest Neighbor Classifier for better effectiveness
from sklearn.neighbors import KNeighborsClassifier
KNN_parkinson = KNeighborsClassifier(n_neighbors=3)
K_Nearest_Neighbor_Classifier = KNN_parkinson.fit(x_train, y_train)
KNN_predict = KNN_parkinson.predict(x_test)
KNN_accuracy = accuracy_score(y_test, KNN_predict)
print("The accuracy score with K Nearest Neighbor Algorithm is:",KNN_accuracy)

#GaussianNB
from sklearn.naive_bayes import GaussianNB
GNB = GaussianNB()
Model_NB = GNB.fit(x_train,y_train)
pred_gnb = Model_NB.predict(x_test)
GNB_accuracy = accuracy_score(y_test, pred_gnb)
print("The accuracy score with Gaussian Naive Bayes is:",GNB_accuracy)

print("\nLet's see the overall accuracy of the built model that is been created below, view the overall accuracy score below!")
Overall_accuracy_percentage = Log_Regression_accuracy+Dec_tree_accuracy+Ran_For_accuracy+Random+Support_accuracy+KNN_accuracy+GNB_accuracy
Average_accuracy = (Overall_accuracy_percentage)/7
print("The accuracy of all the combined metrics for the model is:",Average_accuracy/0.01)

"""####  Converging the above classification algorithms and performance metric using Voting Classifier."""

from sklearn.ensemble import VotingClassifier
VC = VotingClassifier(estimators=[('Classification_model',Classification_model),('Classification_tree',Classification_tree),('Classification_random',Classification_random),('Classification_entropy',Classification_entropy),('Support_vector_machine',Support_vector_machine),('K_Nearest_Neighbor_Classifier',K_Nearest_Neighbor_Classifier),('Model_NB',Model_NB)],voting='hard',flatten_transform=True)
Model_VC = VC.fit(x_train, y_train)
Model_prediction = VC.predict(x_test)
Model_accuracy = accuracy_score(y_test,pred_gnb)
print(Model_accuracy)

"""### XGBClassification - Supervised Machine Learning"""

Model_XG = XGBClassifier(random_state=0)
Model_XG.fit(x_train,y_train)

"""## Assessing the model using metrics"""

y_predict = Model_XG.predict(x_test)
print(accuracy_score(y_test,y_predict)*100)

"""Hence by reducing the overfitting using XGBoost Classifier, we are getting accuracy_score of **98.30%** for the model

#### Confusion metrics
"""

from sklearn.metrics import confusion_matrix
ypre = Classification_model.predict(x_test)
ypre = (ypre>0.5)
confusion_matrix(y_test,ypre)

"""#### F1 score"""

from sklearn.metrics import f1_score
Variation_score = f1_score(y_test, Model_XG.predict(x_test), average='binary')
print(Variation_score/0.01)

"""#### Classification report"""

from sklearn import metrics
from sklearn.metrics import classification_report
print("\n Classification report for Model  %s:\n%s\n" % (Model_XG, metrics.classification_report(y_test, y_pred)))

final_data = parkinson_data.rename(columns = {'MDVP:Fo(Hz)':'Fo','MDVP:Fhi(Hz)':'Fhi','MDVP:Flo(Hz)':'Flo','MDVP:Shimmer(dB)':'Shimmer'})
final_data

"""## Saving the model"""

import pickle

with open( 'Parkinson_MLmodel.sav', 'wb') as f:
    pickle.dump(Model_XG,f)
    
with open('standardScalar.sav', 'wb') as f:
    pickle.dump(Scaler_data,f)